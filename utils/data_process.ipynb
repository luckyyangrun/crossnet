{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "cba48667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype, is_categorical\n",
    "import dgl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "491c773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _series_to_tensor(series):\n",
    "    if is_categorical(series):\n",
    "        return torch.LongTensor(series.cat.codes.values.astype('int64'))\n",
    "    else:       # numeric\n",
    "        return torch.FloatTensor(series.values)\n",
    "\n",
    "class PandasGraphBuilder(object):\n",
    "    \"\"\"Creates a heterogeneous graph from multiple pandas dataframes.\n",
    "    Examples\n",
    "    --------\n",
    "    Let's say we have the following three pandas dataframes:\n",
    "    User table ``users``:\n",
    "    ===========  ===========  =======\n",
    "    ``user_id``  ``country``  ``age``\n",
    "    ===========  ===========  =======\n",
    "    XYZZY        U.S.         25\n",
    "    FOO          China        24\n",
    "    BAR          China        23\n",
    "    ===========  ===========  =======\n",
    "    Game table ``games``:\n",
    "    ===========  =========  ==============  ==================\n",
    "    ``game_id``  ``title``  ``is_sandbox``  ``is_multiplayer``\n",
    "    ===========  =========  ==============  ==================\n",
    "    1            Minecraft  True            True\n",
    "    2            Tetris 99  False           True\n",
    "    ===========  =========  ==============  ==================\n",
    "    Play relationship table ``plays``:\n",
    "    ===========  ===========  =========\n",
    "    ``user_id``  ``game_id``  ``hours``\n",
    "    ===========  ===========  =========\n",
    "    XYZZY        1            24\n",
    "    FOO          1            20\n",
    "    FOO          2            16\n",
    "    BAR          2            28\n",
    "    ===========  ===========  =========\n",
    "    One could then create a bidirectional bipartite graph as follows:\n",
    "    >>> builder = PandasGraphBuilder()\n",
    "    >>> builder.add_entities(users, 'user_id', 'user')\n",
    "    >>> builder.add_entities(games, 'game_id', 'game')\n",
    "    >>> builder.add_binary_relations(plays, 'user_id', 'game_id', 'plays')\n",
    "    >>> builder.add_binary_relations(plays, 'game_id', 'user_id', 'played-by')\n",
    "    >>> g = builder.build()\n",
    "    >>> g.number_of_nodes('user')\n",
    "    3\n",
    "    >>> g.number_of_edges('plays')\n",
    "    4\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.entity_tables = {}\n",
    "        self.relation_tables = {}\n",
    "\n",
    "        self.entity_pk_to_name = {}     # mapping from primary key name to entity name\n",
    "        self.entity_pk = {}             # mapping from entity name to primary key\n",
    "        self.entity_key_map = {}        # mapping from entity names to primary key values\n",
    "        self.num_nodes_per_type = {}\n",
    "        self.edges_per_relation = {}\n",
    "        self.relation_name_to_etype = {}\n",
    "        self.relation_src_key = {}      # mapping from relation name to source key\n",
    "        self.relation_dst_key = {}      # mapping from relation name to destination key\n",
    "\n",
    "    def add_entities(self, entity_table, primary_key, name):\n",
    "        entities = entity_table[primary_key].astype('category')\n",
    "        if not (entities.value_counts() == 1).all():\n",
    "            raise ValueError('Different entity with the same primary key detected.')\n",
    "        # preserve the category order in the original entity table\n",
    "        entities = entities.cat.reorder_categories(entity_table[primary_key].values)\n",
    "\n",
    "        self.entity_pk_to_name[primary_key] = name\n",
    "        self.entity_pk[name] = primary_key\n",
    "        #import ipdb;ipdb.set_trace()\n",
    "        self.num_nodes_per_type[name] = entity_table[primary_key].nunique()\n",
    "        self.entity_key_map[name] = entities\n",
    "        self.entity_tables[name] = entity_table\n",
    "\n",
    "    def add_binary_relations(self, relation_table, source_key, destination_key, name):\n",
    "        src = relation_table[source_key].astype('category')\n",
    "        src = src.cat.set_categories(\n",
    "            self.entity_key_map[self.entity_pk_to_name[source_key]].cat.categories)\n",
    "        dst = relation_table[destination_key].astype('category')\n",
    "        dst = dst.cat.set_categories(\n",
    "            self.entity_key_map[self.entity_pk_to_name[destination_key]].cat.categories)\n",
    "        if src.isnull().any():\n",
    "            raise ValueError(\n",
    "                'Some source entities in relation %s do not exist in entity %s.' %\n",
    "                (name, source_key))\n",
    "        if dst.isnull().any():\n",
    "            raise ValueError(\n",
    "                'Some destination entities in relation %s do not exist in entity %s.' %\n",
    "                (name, destination_key))\n",
    "\n",
    "        srctype = self.entity_pk_to_name[source_key]\n",
    "        dsttype = self.entity_pk_to_name[destination_key]\n",
    "        etype = (srctype, name, dsttype)\n",
    "        #import ipdb;ipdb.set_trace()\n",
    "        self.relation_name_to_etype[name] = etype\n",
    "        self.edges_per_relation[etype] = (src.cat.codes.values.astype('int64'), dst.cat.codes.values.astype('int64'))\n",
    "        self.relation_tables[name] = relation_table\n",
    "        self.relation_src_key[name] = source_key\n",
    "        self.relation_dst_key[name] = destination_key\n",
    "\n",
    "    def build(self):\n",
    "        # Create heterograph\n",
    "        graph = dgl.heterograph(self.edges_per_relation, self.num_nodes_per_type)\n",
    "        return graph\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "d93d74a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rat(filePath, primary_key):\n",
    "    data = []\n",
    "    with open(filePath, 'r') as f:\n",
    "        for line in f:\n",
    "            if line:\n",
    "                lines = line.split(\"\\t\")\n",
    "                user = int(lines[0])\n",
    "                item = int(lines[1])\n",
    "                score = float(lines[2])\n",
    "                data.append({primary_key[0]:int(user), primary_key[1]:int(item), 'score':int(score)})\n",
    "    data = sorted(data, key=lambda x: (x['user_id']))\n",
    "    data = pd.DataFrame(data)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "dd370958",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_filepath = '/Users/yangrun/MyProjects/GA-DTCDR/Data/douban_book/ratings.dat'\n",
    "movie_filepath = '/Users/yangrun/MyProjects/GA-DTCDR/Data/douban_movie/ratings.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "0c41c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "book = read_rat(book_filepath,['user_id', 'book_id'])\n",
    "movie = read_rat(movie_filepath,['user_id', 'movie_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "f2746abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = set(book['user_id'])\n",
    "tmp2 = set(movie['user_id'])\n",
    "user_list = list(tmp1 & tmp2)\n",
    "user_id_table = pd.DataFrame({'user_id':user_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "6df7b3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data = book[book.user_id.isin(user_list)]\n",
    "movie_data = movie[movie.user_id.isin(user_list)]\n",
    "book_id_table = pd.DataFrame({'book_id':book_data['book_id'].unique()})\n",
    "movie_id_table = pd.DataFrame({'movie_id':movie_data['movie_id'].unique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "64bd7bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test_mask(data):\n",
    "    data['train_mask'] = np.ones((len(data),), dtype=np.bool)\n",
    "    data['test_mask'] = np.zeros((len(data),), dtype=np.bool)\n",
    "    def train_test_split(df):\n",
    "        if df.shape[0] > 1:\n",
    "            df.iloc[-1, -1] = True\n",
    "            df.iloc[-1, -2] = False\n",
    "        return df\n",
    "    data = data.groupby(['user_id'] ,group_keys=False).apply(train_test_split).sort_index()\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "33cae024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "builder = PandasGraphBuilder()\n",
    "builder.add_entities(book_id_table, 'book_id', 'book')\n",
    "builder.add_entities(movie_id_table, 'movie_id', 'movie')\n",
    "builder.add_entities(user_id_table, 'user_id', 'user')\n",
    "builder.add_binary_relations(book_data, 'user_id', 'book_id', 'rate')\n",
    "builder.add_binary_relations(book_data, 'book_id', 'user_id', 'rated-by')\n",
    "#builder.add_binary_relations(movie_data, 'user_id', 'movie_id', 'rate')\n",
    "#builder.add_binary_relations(movie_data, 'movie_id', 'user_id', 'rated-by')\n",
    "builder.add_binary_relations(movie_data, 'user_id', 'movie_id', 'view')\n",
    "builder.add_binary_relations(movie_data, 'movie_id', 'user_id', 'view-by')\n",
    "g = builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "f10a9509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_entity_dict(entity_table, primary_key):\n",
    "    entities = entity_table[primary_key].astype('category')\n",
    "    entities = entities.cat.reorder_categories(entity_table[primary_key].values)\n",
    "    entity_dict = {k:v for v, k in entities.items()}\n",
    "    return entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "b7239a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id_dict = gen_entity_dict(book_id_table, 'book_id')\n",
    "movie_id_dict = gen_entity_dict(movie_id_table, 'movie_id')\n",
    "user_id_dict = gen_entity_dict(user_id_table, 'user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "31561295",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-348-bdf3227e0ee3>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  book_data['book_id'] = book_data['book_id'].map(book_id_dict)\n",
      "<ipython-input-348-bdf3227e0ee3>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  movie_data['movie_id'] = movie_data['movie_id'].map(movie_id_dict)\n",
      "<ipython-input-348-bdf3227e0ee3>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  book_data['user_id'] = book_data['user_id'].map(user_id_dict)\n",
      "<ipython-input-348-bdf3227e0ee3>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  movie_data['user_id'] = movie_data['user_id'].map(user_id_dict)\n"
     ]
    }
   ],
   "source": [
    "book_data['book_id'] = book_data['book_id'].map(book_id_dict)\n",
    "movie_data['movie_id'] = movie_data['movie_id'].map(movie_id_dict)\n",
    "book_data['user_id'] = book_data['user_id'].map(user_id_dict)\n",
    "movie_data['user_id'] = movie_data['user_id'].map(user_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "28919a69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-344-847c0edfe6f2>:2: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  data['train_mask'] = np.ones((len(data),), dtype=np.bool)\n",
      "<ipython-input-344-847c0edfe6f2>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['train_mask'] = np.ones((len(data),), dtype=np.bool)\n",
      "<ipython-input-344-847c0edfe6f2>:3: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  data['test_mask'] = np.zeros((len(data),), dtype=np.bool)\n",
      "<ipython-input-344-847c0edfe6f2>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['test_mask'] = np.zeros((len(data),), dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "book_data = gen_test_mask(book_data)\n",
    "movie_data = gen_test_mask(movie_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "9c26ea70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yangrun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import nltk\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "\n",
    "MOVIE_FILE = '/Users/yangrun/Downloads/douban_datasettext/douban_dataset(text information)/movies_cleaned.txt'\n",
    "BOOK_REVIEW_FILE = '/Users/yangrun/Downloads/douban_datasettext/douban_dataset(text information)/bookreviews_cleaned.txt'\n",
    "BOOK_RATE = '/Users/yangrun/MyProjects/GA-DTCDR/Data/douban_book/ratings.dat'\n",
    "\n",
    "USER_FILE = '/Users/yangrun/Downloads/douban_datasettext/douban_dataset(text information)/users_cleaned.txt'\n",
    "nlp = StanfordCoreNLP('/Users/yangrun/Downloads/stanford-nlp/stanford-corenlp-4.2.2',lang='zh')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def is_chinese(uchar):\n",
    "    \"\"\"is this a chinese word?\"\"\"\n",
    "    if uchar >= u'\\u4e00' and uchar <= u'\\u9fa5':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_number(uchar):\n",
    "    \"\"\"is this unicode a number?\"\"\"\n",
    "    if uchar >= u'\\u0030' and uchar <= u'\\u0039':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def is_alphabet(uchar):\n",
    "    \"\"\"is this unicode an English word?\"\"\"\n",
    "    if (uchar >= u'\\u0041' and uchar <= u'\\u005a') or (uchar >= u'\\u0061' and uchar <= u'\\u007a'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def format_str(content,lag=1):\n",
    "    #print(content)\n",
    "    content_str = ''\n",
    "    if lag==0: #English\n",
    "       for i in content:\n",
    "           if is_alphabet(i):\n",
    "               content_str = content_str+i\n",
    "    if lag==1: #Chinese\n",
    "        for i in content:\n",
    "            if is_chinese(i):\n",
    "                content_str = content_str+i\n",
    "    if lag==2: #Number\n",
    "        for i in content:\n",
    "            if is_number(i):\n",
    "                content_str = content_str+i        \n",
    "    return content_str\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "91c79bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = pd.read_csv(USER_FILE,sep='\\t')\n",
    "user['self_statement'] = user['self_statement'].fillna('缺失')\n",
    "user['living_place'] = user['living_place'].fillna('缺失')\n",
    "user = user.loc[user.UID.isin(user_list)]\n",
    "user['UID'] = user['UID'].astype('category')\n",
    "user['UID']= user['UID'].cat.reorder_categories(user_id_table['user_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "04118a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "user['UID'] = user['UID'].astype('category')\n",
    "user = user.loc[user.UID.isin(user_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "555d4e2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for uid , text1, text2 in zip(user['UID'], user['self_statement'], user['living_place']):\n",
    "    str_cleaned = ''\n",
    "    str_cleaned=format_str(text2,1)+format_str(text1,1)+str_cleaned\n",
    "    words= nlp.word_tokenize(str_cleaned)\n",
    "    res.append([uid,words])\n",
    "\n",
    "    \n",
    "documents = [TaggedDocument(tags=[str(i)],words=doc) for i, doc in res]\n",
    "user_model = Doc2Vec(documents, vector_size=16, window=2, min_count=1,negative=30, workers=6)\n",
    "#model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "32ce97a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
     ]
    }
   ],
   "source": [
    "user_model.train(documents,total_examples=user_model.corpus_count, epochs=20)\n",
    "user_model.save(\"Doc2vec_douban_user_16.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "7e1b3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_info = pd.read_csv(MOVIE_FILE,sep='\\t')\n",
    "movie_info['UID'] = movie_info['UID'].astype('category')\n",
    "#tmp_dict = {v:int(k) for k, v in movie_id_dict.items()}\n",
    "movie_info['UID'] = movie_info['UID'].map(movie_id_dict)\n",
    "movie_info = movie_info.dropna(subset=['UID'])\n",
    "movie_info['UID'] = movie_info['UID'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "1e6cfad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9555/9555 [02:52<00:00, 55.53it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in [1,2,3,4,5,7,11]:\n",
    "    tag = movie_info.columns[i]\n",
    "    movie_info[tag] = movie_info[tag].fillna('缺失')\n",
    "    movie_info[tag] = movie_info[tag].apply(lambda x :format_str(x))\n",
    "    \n",
    "    res = []\n",
    "    #import pdb;pdb.set_trace()\n",
    "for uid , text1, text2, text3, text4, text5, text6, text7 in tqdm.tqdm(list(zip(movie_info['UID'], movie_info['name'], movie_info['director'],movie_info['summary'], movie_info['writer'], movie_info['country'], movie_info['language'], movie_info['tag']))):\n",
    "    str_cleaned = ''\n",
    "    str_cleaned=format_str(text1)+format_str(text2)+format_str(text3)+ format_str(text4)+format_str(text5)+format_str(text6)+format_str(text7)+str_cleaned\n",
    "    words= nlp.word_tokenize(str_cleaned)\n",
    "    res.append([uid,words])\n",
    "documents = [TaggedDocument(tags=[str(i)],words=doc) for i, doc in res]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "4ef054a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in (set(movie_id_dict.keys()) - set(movie_info['UID'].values)):\n",
    "    documents.append(TaggedDocument(tags=[str(i)], words=['缺失值']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "997e4a11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
     ]
    }
   ],
   "source": [
    "movie_model = Doc2Vec(documents, vector_size=16, window=2, min_count=1,negative=30, workers=6)\n",
    "movie_model.train(documents,total_examples=movie_model.corpus_count, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "f2229196",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_model.save(\"Doc2vec_douban_movie_16.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "7410bb06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-314-e8f0bfa5f907>:2: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  key2index = movie_model.docvecs.key_to_index\n",
      "<ipython-input-314-e8f0bfa5f907>:4: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  movie_feature.append(movie_model.docvecs.vectors[key2index[str(key)]])\n"
     ]
    }
   ],
   "source": [
    "movie_feature = []\n",
    "key2index = movie_model.docvecs.key_to_index\n",
    "for key in movie_id_dict.keys():\n",
    "    movie_feature.append(movie_model.docvecs.vectors[key2index[str(key)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "eb30ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_rated = set()\n",
    "with open(BOOK_RATE , 'r') as f:\n",
    "    for line in f:\n",
    "        if line:\n",
    "            lines = line.split(\"\\t\")\n",
    "            item = int(lines[1])\n",
    "            if item in book_id_dict.keys():\n",
    "                book_rated.add(book_id_dict[item])\n",
    "\n",
    "book_document = {}\n",
    "with open(BOOK_REVIEW_FILE, 'r') as f:\n",
    "    lines = f.readlines()[1:]\n",
    "for line in lines:\n",
    "    cur = line.split(\"\\t\")\n",
    "    book_id = int(cur[1].strip('\"\"'))\n",
    "    label = str(cur[3].strip('\"\"'))\n",
    "    review = str(cur[4].strip('\"\"'))\n",
    "    if not label : label = review\n",
    "    str_cleaned = ''\n",
    "    str_cleaned = format_str(label)+str_cleaned\n",
    "    if book_id in book_rated:\n",
    "        book_document[book_id] = str_cleaned\n",
    "        \n",
    "#6777缺失值   \n",
    "book_document[6777] = '缺失值'\n",
    "documents = [TaggedDocument(tags=[str(i)],words=doc) for i, doc in book_document.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "44280e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.doc2vec:Each 'words' should be a list of words (usually unicode strings). First 'words' here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
     ]
    }
   ],
   "source": [
    "book_model = Doc2Vec(documents, vector_size=16, window=2, min_count=1,negative=30, workers=6)\n",
    "book_model.train(documents,total_examples=book_model.corpus_count, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "e2f15113",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_model.save(\"Doc2vec_douban_book_16.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "fd4568e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-318-fde05ae48309>:4: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  key2index = book_model.docvecs.key_to_index\n",
      "<ipython-input-318-fde05ae48309>:5: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  book_feature.append(book_model.docvecs.vectors[key2index[str(key)]])\n"
     ]
    }
   ],
   "source": [
    "book_feature = []\n",
    "for key in book_id_dict.keys():\n",
    "    #print(key)\n",
    "    key2index = book_model.docvecs.key_to_index\n",
    "    book_feature.append(book_model.docvecs.vectors[key2index[str(key)]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "978777fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "book_test = book_data[book_data['test_mask']==True]\n",
    "movie_test = movie_data[movie_data['test_mask']==True]\n",
    "#book_test['book_id'] = book_test['book_id'].map(book_id_dict)\n",
    "#book_test['user_id'] = book_test['user_id'].map(user_id_dict)\n",
    "#movie_test['movie_id'] = movie_test['movie_id'].map(movie_id_dict)\n",
    "#movie_test['user_id'] = movie_test['user_id'].map(user_id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "aad18744",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1_test = set(book_test['user_id'])\n",
    "tmp2_test = set(movie_test['user_id'])\n",
    "user_list_test = list(tmp1_test & tmp2_test)\n",
    "book_test = book_test[book_test.user_id.isin(user_list_test)]\n",
    "movie_test = movie_test[movie_test.user_id.isin(user_list_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "609e4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for etype in [('user','rate','book'), ('book', 'rated-by','user')]:\n",
    "#    edge_mask = torch.ones(g.number_of_edges(etype))\n",
    "#    for u,v in zip(book_test['user_id'], book_test['book_id']):\n",
    "#        eidx = g.edge_ids(u,v,etype = etype)\n",
    "mask = {}\n",
    "edge_mask = torch.ones(g.number_of_edges(('user','rate','book')))\n",
    "for u,v in zip(book_test['user_id'], book_test['book_id']):\n",
    "    eidx = g.edge_ids(u,v,etype = ('user','rate','book'))\n",
    "    edge_mask[eidx]=0\n",
    "mask[('user','rate','book')] = edge_mask.bool()\n",
    "inv_edge_mask = torch.ones(g.number_of_edges(('book', 'rated-by','user')))\n",
    "for u,v in zip(book_test['book_id'], book_test['user_id']):\n",
    "    eidx = g.edge_ids(u,v,etype = ('book', 'rated-by','user'))        \n",
    "    inv_edge_mask[eidx]=0\n",
    "mask[('book', 'rated-by','user')] = inv_edge_mask.bool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "ad6b3017",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#edge_mask = torch.ones(g.number_of_edges(('user','rate','movie')))\n",
    "#for u,v in zip(movie_test['user_id'], movie_test['movie_id']):\n",
    "#    eidx = g.edge_ids(u,v,etype = ('user','rate','movie'))\n",
    "#    edge_mask[eidx]=0\n",
    "#mask[('user','rate','movie')] = edge_mask.bool()\n",
    "#\n",
    "#inv_edge_mask = torch.ones(g.number_of_edges(('movie', 'rated-by','user')))\n",
    "#for u,v in zip(movie_test['movie_id'], movie_test['user_id']):\n",
    "#    eidx = g.edge_ids(u,v,etype = ('movie', 'rated-by','user'))        \n",
    "#    inv_edge_mask[eidx]=0\n",
    "#mask[('movie', 'rated-by','user')] = inv_edge_mask.bool()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "edge_mask = torch.ones(g.number_of_edges(('user','view','movie')))\n",
    "for u,v in zip(movie_test['user_id'], movie_test['movie_id']):\n",
    "    eidx = g.edge_ids(u,v,etype = ('user','view','movie'))\n",
    "    edge_mask[eidx]=0\n",
    "mask[('user','view','movie')] = edge_mask.bool()\n",
    "\n",
    "inv_edge_mask = torch.ones(g.number_of_edges(('movie', 'view-by','user')))\n",
    "for u,v in zip(movie_test['movie_id'], movie_test['user_id']):\n",
    "    eidx = g.edge_ids(u,v,etype = ('movie', 'view-by','user'))        \n",
    "    inv_edge_mask[eidx]=0\n",
    "mask[('movie', 'view-by','user')] = inv_edge_mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "478c7a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'book': 6777, 'movie': 9555, 'user': 2106},\n",
       "      num_edges={('book', 'rated-by', 'user'): 94016, ('movie', 'view-by', 'user'): 967979, ('user', 'rate', 'book'): 94016, ('user', 'view', 'movie'): 967979},\n",
       "      metagraph=[('book', 'user', 'rated-by'), ('user', 'book', 'rate'), ('user', 'movie', 'view'), ('movie', 'user', 'view-by')])"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl.edge_subgraph(g, mask, preserve_nodes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "baef4370",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl.edge_subgraph(g, mask, preserve_nodes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "8671579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_train = book_data[book_data['test_mask']==False]\n",
    "movie_train = movie_data[movie_data['test_mask']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "74b7e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_trainDict = zip(book_train['user_id'].values, book_train['book_id'].values)\n",
    "movie_trainDict = zip(movie_train['user_id'].values, movie_train['movie_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "66e83a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_size = book_data['book_id'].nunique()\n",
    "movie_size = movie_data['movie_id'].nunique()\n",
    "user = []\n",
    "item_a = []\n",
    "item_b = []\n",
    "for u, v in zip(book_test['user_id'], book_test['book_id']):\n",
    "    tmp_user = u\n",
    "    tmp_item = []\n",
    "    #import ipdb;ipdb.set_trace()\n",
    "    item_id = v\n",
    "    tmp_item.append(item_id)\n",
    "    neglist = set()\n",
    "    neglist.add(item_id)\n",
    "    for t in range(99):\n",
    "        j = np.random.randint(book_size)\n",
    "        while (u, j) in book_trainDict or j in neglist:\n",
    "            j = np.random.randint(book_size)\n",
    "        neglist.add(j)\n",
    "        tmp_item.append(j)\n",
    "    item_b.append(tmp_item)\n",
    "    user.append(tmp_user)\n",
    "    \n",
    "for u, v in zip(movie_test['user_id'], movie_test['movie_id']):\n",
    "    tmp_user = u\n",
    "    tmp_item = []\n",
    "    #import ipdb;ipdb.set_trace()\n",
    "    item_id = v\n",
    "    tmp_item.append(item_id)\n",
    "    neglist = set()\n",
    "    neglist.add(item_id)\n",
    "    for t in range(99):\n",
    "        j = np.random.randint(movie_size)\n",
    "        while (u, j) in movie_trainDict or j in neglist:\n",
    "            j = np.random.randint(book_size)\n",
    "        neglist.add(j)\n",
    "        tmp_item.append(j)\n",
    "    item_a.append(tmp_item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "cbafd564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-361-0364551bb6a0>:1: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  g.ndata['s2v'] = {'user':torch.tensor(user_model.docvecs.vectors),\n"
     ]
    }
   ],
   "source": [
    "g.ndata['s2v'] = {'user':torch.tensor(user_model.docvecs.vectors),\n",
    "                     'movie':torch.from_numpy(np.array(movie_feature)),\n",
    "                     'book': torch.from_numpy(np.array(book_feature))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "ee19d1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {'train_graph':g,\n",
    "          'test_user':user,\n",
    "          'test_item_a':item_a,\n",
    "          'test_item_b':item_b}\n",
    "with open('./data_s2v_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "cfad250b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2, 16])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand((100,16))\n",
    "b = torch.rand((100,16))\n",
    "torch.stack([a,b]).permute(1,0,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "807962b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'dict' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-363-eba56fa90e9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m{\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'dict' and 'dict'"
     ]
    }
   ],
   "source": [
    "{'a':1,'b':2} + {'c':2,'d':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "1d56de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {'a': 1, 'b': 2, 'b': '3'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "90a7d73b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't use starred expression here (<ipython-input-365-8441f717ac2c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-365-8441f717ac2c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    *dict\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't use starred expression here\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381deab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
